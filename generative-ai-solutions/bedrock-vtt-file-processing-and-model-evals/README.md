# Meeting transcript chapter title generation and comparison across FMs with Amazon Bedrock

This example shows how to generate chapter titles for [`Video Text to track (VTT)`](https://en.wikipedia.org/wiki/WebVTT) files using different Foundation Models (FMs) available in Bedrock and then evaluate the quality of the generated titles. The evaluation is done by comparing the titles generated by FMs with human written titles if available using quantitative metrics such as [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)#:~:text=The%20metrics%20compare%20an%20automatically,produced%20summary%20and%20the%20reference.) and [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) Scores as well as by using an _LLM as a judge_ inspired by this [paper](https://arxiv.org/abs/2306.05685). The _LLM as a judge_ approach that uses an [evaluation prompt template](data/prompts/eval_template.txt) provides a scalable _human out of the loop_ mechanism for evaluating FM outputs.

Additionally, this repo also shows how to use [LiteLLM](https://github.com/BerriAI/litellm) for interfacing with Bedrock and [Ray](https://github.com/ray-project/ray) for running Bedrock inference concurrently in an asynchronous manner.

## Workflow

The following steps describe how this solution works.

### Data Preparation - Option 1 - Use the provided VTT files

1. We provide a synthetic dataset of a couple of VTT files to illustrate the use of this repo. Replace the VTT files in the [`data/source_data`](./data/source_data/) folder with your own VTT files to test with your own dataset. Here is a snippet of the synthetically generated VTT file used for this repo.

    ```{.markdown}
    WEBVTT

    1 "Dr. E" (1234567890)
    00:00:00.000 --> 00:00:05.340
    Have you all seen the latest results from the Large Hadron Collider? The data on the Higgs boson is quite fascinating.

    2 "Dr. M" (2345678901)
    00:00:05.340 --> 00:00:10.720
    Yes, I've been studying the particle trajectories closely. The decay patterns are consistent with our theoretical models.

    3 "Dr. S" (3456789012)
    00:00:10.720 --> 00:00:16.500
    I'm more interested in the potential applications of these findings. Could we harness the Higgs field for energy production?

    4 "Dr. D" (4567890123)
    00:00:16.500 --> 00:00:21.870
    That's an intriguing idea, but we'd need to overcome some significant technical hurdles first.

    5 "Dr. E" (1234567890)
    00:00:21.870 --> 00:00:27.140
    Absolutely. Have you considered the implications of these findings on our understanding of dark matter?
    ```

1. Chapterize the VTT files, this is done by simply splitting the transcript into chunks of 25 lines each (configurable). You can replace the chapterization logic in the [`0_chapterize_data`](./0_chapterize_data.ipynb) notebook with your own custom logic. The chapterized data from each transcript is stored in a single CSV file [`chapterized.csv`](./data/processed_data/chapterized.csv) file.

### Data Preparation - Option 2 - Use your own chapterized JSON files

1. If you have a json file that is already chapterized in the following format, run the [0_already_chapterized_data.ipynb](0_already_chapterized_data.ipynb) that sends the `already chapterized` data as a dataframe for model title generation:

    ```{.json}
    {
        "chapters": [
            {
            "chapter_1": ,
            "chapter_2": ,
             ...
            }
        ]
        "original transcript": [
            {
                ...
            }
        ]
        }
    ```

### Chapter Title Generation

1. Run each chapter through the models of interest using LiteLLM and Bedrock and track metrics such as `inference latency`, `prompt and generation tokens` and `cost per transaction`. The completions and metrics for each chapter title generation is stored in a JSON file in the [`data/title_completions`](./data/title_completions/) folder.

1. Finally, we create a set of summarized results which provide the `mean` and `p95` metrics for each of the models under test and a human readable summary for each model which can help decide which model to pick for your specific dataset. Here is an example of the summarized results file generated by this solution.

    >_The numbers presented in the table below are just examples and subject to change. Please test with your own dataset to get actual numbers for your dataset of interest_.

    | model_id | latency_seconds | completion_token_count | prompt_token_count | input_token_price | output_token_pricing | p95_latency_seconds | avg_cost_per_txn | p95_cost_per_txn | p95_completion_token_count | p95_prompt_token_count | count | overall_report |
    | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
    | amazon.titan-text-express-v1 | 1.214417 | 8 | 480 | 0.000384 | 1.3e-05 | 1.370577 | 0.000397 | 0.000518 | 12.25 | 623.3 | 4 | The average inference latency for this workload with prompt tokens 480 (p95 is 623) and completion tokens 8 (p95 is 12) when using amazon.titan-text-express-v1 is 1.2144s (p95 is 1.3706s) and the average cost per-request is $0.000397 (p95 is $0.000518), this is based on 4 requests. |

### Model and Title Evaluation

1. We divide the evaluation step into two parts:

    - In the first part, we calculate the **_ROUGE & Cosine Similarity_** scores as metrics to show the most optimal model from a quantitative perspective.

    - In the second part, we use **Claude Sonnet** as an **_LLM that acts as a judge_** for subjective evaluation, and based on an evaluation prompt you define, it will generate the most optimal titles generated by a "selected model" that it shortlists, along with an "explanation" of why it chose that given title generated by the given **_selected model_** over other titles and models.

1. Following is an example of an **_evaluation prompt_** that Claude as a Judge uses to make a decision on the most optimal title generated:

    ```{.raw}

    Human: here is a transcript from a meeting in the <chapter></chapter> tag followed by chapter titles generated by different models. Your task is to select the title that best captures the content and meaning of the chapter in 1 to 4 words. Put the selected title, model name and explanation for selecting the title and not selecting other titles in a JSON as within 3 elements:  "selected_title", "selected_model", "explanation".

    Your explanation should include both model name and title so that it is simple to understand which title was generated by which model and why it was or was not selected.

    <chapter>
    {chapter_text}
    </chapter>

    <model_x> # human generated title referred to as model 'X' to view how it performs against the other models
    {original_title}
    </model_x>

    {model_titles}

    Assistant: Here is the response in json:

    ```

## Contents

The example consists of three Jupyter notebooks and a couple of Python scripts:

- [`0_chapterize_data.ipynb`](./0_chapterize_data.ipynb) - This notebook contains the data preparation code. Parses the transcripts and chapterized them.
- [`0_already_chapterized_data.ipynb`](./0_already_chapterized_data.ipynb) - This notebook contains the data preparation code. Parses already chapterized data.

- [`1_generate_chapter_titles.ipynb`](./1_generate_chapter_titles.ipynb) - This notebook uses LiteLLM and Bedrock to generate chapter titles and stores the results and metrics in a JSON file.

- [`2_summarize_metrics.ipynb`](./2_summarize_metrics.ipynb) - This notebook summarizes the metrics and creates the results in human readable format. It gives quantitative (**_ROUGE & Cosine Similarity_**) and Subjective (**_LLM acts as a Judge_**) metrics to get the most optimal model to be used for title generation.

- [`main.py`](./main.py) - Script to run all the notebooks through a single command. See section on `Running`.

- [`config.yml`](./config.yml) - contains configuration parameters such as directory path, model information etc. for this solution. ***The pricing information in the [`config.yml`](./config.yml) is subject to change, please always confirm Bedrock pricing from the [`Amazon Bedrock Pricing`](https://aws.amazon.com/bedrock/pricing/) page***.

### Dataset

The dataset used in this repo is a synthetically dataset generated through the Bedrock chat playground using the Anthropic Claude 3 Sonnet model with the following prompt. It has been manually edits to get into the exact VTT format.

```{.raw}

Human: create a meeting conversation transacript in the VTT format described here https://en.wikipedia.org/wiki/WebVTT between a group of 5 particle physicists. Generate a 1000 line conversation. Call the physicists Dr. A, Dr. B, Dr. C and so on.

Assistant:
```

## Setup

It is best to create a separate conda environment to run this solution using the commands provided below:

```
conda create --name model_eval_bedrock_py311 -y python=3.11 ipykernel
conda activate model_eval_bedrock_py311
pip install -r requirements.txt -U
```

Use the `model_eval_bedrock_py311` conda environment for all notebooks in this folder.

## Running

Run the following command which will run all the notebooks included in this repo in the correct order.

```{.bash}
rm -rf data/metrics data/title_completions
python main.py
```

You could also run the notebooks manually in the order listed below:

1. [`0_chapterize_data.ipynb`](./0_chapterize_data.ipynb) OR  [`0_already_chapterized_data.ipynb`](./0_already_chapterized_data.ipynb)
1. [`1_generate_chapter_titles.ipynb`](./1_generate_chapter_titles.ipynb)
1. [`2_summarize_metrics.ipynb`](./2_summarize_metrics.ipynb)

### Bring your own chapterization logic

The current solution uses a naive approach to chapterization by treating every Nth (=25, configurable) statement as a chapter boundary. You can easily replace this logic by adding your custom logic in the [`chapterize.py`](./chapterize.py) script.
