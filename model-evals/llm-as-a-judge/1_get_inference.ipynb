{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa5bc74-7b05-46dd-9c6b-2f6195942c3b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Step 1: Invoke Bedrock Models to get _Inferences_ on a user provided Dataset\n",
    "---\n",
    "\n",
    "This notebook does as follows:\n",
    "\n",
    "1. Generates inferences on a user provided dataset, using Foundation models on Amazon Bedrock\n",
    "\n",
    "1. Uses [Litellm](https://www.litellm.ai/) as an interface to interact with the Bedrock API\n",
    "\n",
    "1. Uses `Ray`, which is used to run inferences in an asynchronous manner\n",
    "\n",
    "1. Records metrics like the `p90, p95` latency, `prompt token counts`, `completion token counts`, and more.\n",
    "\n",
    "1. Saves all the combined _model responses_ to user questions from the source dataset in a `all_results.csv` that is used later in the _evaluation step_ for the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3d35d-a54b-4b57-8562-dac1ed1b51ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import ray\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from litellm import completion\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be5469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a51143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a77295-6987-49dd-baaf-7dc0957f1981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n",
    "\n",
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fe3ec-1987-4fe0-9a02-cd2cc98cfac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize all global variables that are used across this notebook hydrated from the `config.yaml` file\n",
    "\n",
    "# name of your source xlsx/xls/csv file \n",
    "FILE_NAME: str = config['dir_info']['dataset_file_name']\n",
    "# data directory\n",
    "DATA_DIR: str = config['dir_info']['data_dir']\n",
    "FILE_RELATIVE_PATH: str = os.path.join(config['dir_info']['dataset_dir'], FILE_NAME)\n",
    "INPUT_FPATH: str = os.path.join(DATA_DIR, FILE_RELATIVE_PATH)\n",
    "USER_PROMPT_COL: str = config['dataset_info']['user_question_col']\n",
    "SYSTEM_PROMPT_COL: str = config['dataset_info']['system_prompt_col']\n",
    "INFERENCE_PARAMETERS: Dict = config['inference_parameters']\n",
    "ON_LIST = list(filter(None, [USER_PROMPT_COL, \n",
    "                            SYSTEM_PROMPT_COL]))\n",
    "TEXT_WRAP_WIDTH_SIZE: int = 80\n",
    "\n",
    "# result files\n",
    "ALL_RESULTS_FPATH = os.path.join(DATA_DIR, config['dir_info']['metrics'])\n",
    "INFERENCE_LATENCY_SUMMARY_FPATH = os.path.join(ALL_RESULTS_FPATH, config['dir_info']['inference_latency_summary_fname'])\n",
    "bedrock_model_ids: List[str] = config['bedrock_fms_to_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c428541",
   "metadata": {},
   "source": [
    "### Get Inference for the given dataset\n",
    "---\n",
    "\n",
    "This portion of the notebook gets inference using `Ray` (which is used to handle asynchronous calls to `Litellm`) to get inferences from the user questions in the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0eda48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_task_inference(model_id: str, \n",
    "                            user_prompt: str, \n",
    "                            system_prompt: str, \n",
    "                            inference_parameters: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    This function takes in a dictionary (which contains information on the user data and prompts) \n",
    "    to generate inference using a bedrock model id, and returns a dictionary containing the model \n",
    "    completion, and latency (in seconds).\n",
    "    \"\"\"\n",
    "    # represents the service name\n",
    "    service_name: str = \"bedrock\"\n",
    "    temperature = inference_parameters.get('temperature', 0.1)\n",
    "    caching = inference_parameters.get('caching', False)\n",
    "    max_tokens = inference_parameters.get(\"max_tokens\", 500)\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name\n",
    "    # initialize the response dict\n",
    "    ret = dict(user_prompt=user_prompt,\n",
    "               system_prompt=system_prompt,\n",
    "               completion=None,\n",
    "               model_id=model_id,\n",
    "               time_taken_in_seconds=None,\n",
    "               prompt_token_count=None,\n",
    "               completion_token_count=None,\n",
    "               exception=None)\n",
    "    # custom messages formatting for when the user/system roles are given together\n",
    "    messages=[{ \"content\": user_prompt, \"role\": \"user\"}]\n",
    "    if system_prompt is not None:\n",
    "        messages.append({\"content\": system_prompt, \"role\": \"system\"})\n",
    "\n",
    "    # set the env var for aws_region\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region \n",
    "    try:\n",
    "        print(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=messages,\n",
    "                              temperature=temperature,\n",
    "                              max_tokens=max_tokens,\n",
    "                              caching=caching)\n",
    "\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            print(f\"choice {idx+1} of {len(response.choices)} \")\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "\n",
    "        # Commenting out the code below that records the number of input and output tokens.\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        # Extract latency in seconds\n",
    "        latency_ms = response._response_ms\n",
    "        ret['time_taken_in_seconds']  = latency_ms / 1000\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833da87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def async_generate_task_inference(input: Dict, model_id: str, inference_parameters: Dict) -> Dict:\n",
    "    resp = generate_task_inference(model_id, input.get('user_prompt'), input.get('system_prompt'), inference_parameters)\n",
    "    resp_this_model = {\"model_id\": model_id,\n",
    "                       USER_PROMPT_COL: input.get('user_prompt'),\n",
    "                       SYSTEM_PROMPT_COL: input.get('system_prompt'),\n",
    "                       f\"{model_id}-response\": resp['completion'],\n",
    "                       f\"{model_id}-time_taken_in_seconds\": resp['time_taken_in_seconds'],\n",
    "                       f\"{model_id}-prompt_token_count\": resp['prompt_token_count'],\n",
    "                       f\"{model_id}-completion_token_count\": resp['completion_token_count'],\n",
    "                       f\"{model_id}-exception\": resp['exception']}\n",
    "    return resp_this_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8840b-776f-4a3f-bcc3-dd84bcc46829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"File name to be processed: {INPUT_FPATH}\")\n",
    "data_file = Path(INPUT_FPATH)\n",
    "if data_file.suffix == '.csv':\n",
    "    logger.info(f\"processing the csv file: {data_file}\")\n",
    "    original_eval_df = pd.read_csv(data_file)\n",
    "elif data_file.suffix in ['.xls', '.xlsx']:\n",
    "    logger.info(f\"processing the xls/xlsx file: {data_file}\")\n",
    "    original_eval_df = pd.read_excel(data_file)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file format: {data_file.suffix}\")\n",
    "logger.info(f\"input data frame shape is {original_eval_df.shape}\")\n",
    "# drop the columns that have all 'NaN' values\n",
    "original_eval_df = original_eval_df.dropna(axis=1, how='all')\n",
    "original_eval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7566268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_eval_list = json.loads(original_eval_df.to_json(orient='records'))\n",
    "original_eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f71ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of the bedrock model ids that are used in generating inferences\n",
    "bedrock_model_ids: List[str] =[d['model_id'] for d in config['bedrock_fms_to_test']]\n",
    "bedrock_model_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc7655",
   "metadata": {},
   "source": [
    "### Run the inferences to get model responses in parallel using `Ray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadf913-7199-42fd-b569-e489ac9857b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "erroneous_count: int = 0\n",
    "resp_list = []\n",
    "n: int = config.get('parallel_inference_count')\n",
    "st_overall = time.perf_counter()\n",
    "# Iterate over each bedrock model ID\n",
    "for model_id in bedrock_model_ids:\n",
    "    logger.info(f\"going to get inference from model={model_id}\")\n",
    "    list_of_lists = [original_eval_list[i * n:(i + 1) * n] for i in range((len(original_eval_list) + n - 1) // n )]\n",
    "    st = time.perf_counter()\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        logger.info(f\"processing sublist={idx+1}/{len(list_of_lists)} for model_id={model_id}\")\n",
    "        for input in sublist:\n",
    "            print(f\"input logged: {input}\")\n",
    "            try:\n",
    "                input_dict = dict(user_prompt=input.get(USER_PROMPT_COL), system_prompt=input.get(SYSTEM_PROMPT_COL))\n",
    "                result = ray.get(async_generate_task_inference.remote(input_dict, model_id, INFERENCE_PARAMETERS))\n",
    "                resp_list.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing input: {input} for model_id={model_id}, error: {e}\")\n",
    "                erroneous_count += 1\n",
    "\n",
    "    elapsed_time = time.perf_counter() - st\n",
    "    logger.info(f\"total time taken for {len(original_eval_list)} with model={model_id} is {elapsed_time:0.2f}\")\n",
    "elapsed_time = time.perf_counter() - st_overall\n",
    "logger.info(f\"total time taken for {len(original_eval_list)} with models={bedrock_model_ids} is {elapsed_time:0.2f}\")\n",
    "logger.info(f\"total erroneous count: {erroneous_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa6bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view some responses generated\n",
    "resp_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f7852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for model_id in bedrock_model_ids:\n",
    "    df_list.append(pd.DataFrame([r for r in resp_list if r['model_id'] == model_id]).drop(['model_id'], axis=1))    \n",
    "from functools import reduce\n",
    "logger.info(f\"on_list: {ON_LIST}\")\n",
    "try:\n",
    "    # if the system prompt is separately provided, merge on that column too else, just use the user\n",
    "    # column for the merge\n",
    "    df_resp = reduce(lambda x, y: pd.merge(x, y, on=ON_LIST), \n",
    "                    df_list)\n",
    "except Exception as e:\n",
    "    logger.error(f\"df was not merged: {e}\")\n",
    "logger.info(f\"shape of response data frame={df_resp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ecf042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the data frame\n",
    "df_resp = df_resp.loc[:, ~df_resp.columns.str.startswith('None')]\n",
    "df_resp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad630fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original/target responses if any and merge it with the current df\n",
    "try: \n",
    "    if df_resp is not None and config['dataset_info']['pre_existing_response_col'] is not None:\n",
    "        df_resp_all = pd.merge(left=df_resp, right=original_eval_df, how=\"left\",\n",
    "                            left_on=ON_LIST, \n",
    "                            right_on=ON_LIST)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not perform the merge with the original data frame: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the current data in the df\n",
    "df_resp_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82edfdcf",
   "metadata": {},
   "source": [
    "### Record the `p50` and `p95` inference latencies in a `txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_in_seconds_cols = [c for c in df_resp_all.columns if 'time_taken_in_seconds' in c]\n",
    "Latency_cols = [c for c in df_resp_all.columns if 'Latency ' in c]\n",
    "all_latency_cols_of_interest = time_taken_in_seconds_cols + Latency_cols\n",
    "summary = \"\"\n",
    "for c in all_latency_cols_of_interest:\n",
    "    quantiles = list(round(df_resp_all[c].quantile([0.5, 0.95]), 2))\n",
    "    s = f\"[p50, p95] for {c}={quantiles}\\n\"\n",
    "    summary += s\n",
    "    logger.info(s)\n",
    "Path(INFERENCE_LATENCY_SUMMARY_FPATH).write_text(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510784d",
   "metadata": {},
   "source": [
    "### Upload the overall results to a `results.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f992e2-d229-4cab-84d6-b5eff23bbedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = list(df_resp_all.columns)\n",
    "user_input_index = df_resp_all.columns.get_loc(config['dataset_info']['user_question_col'])\n",
    "response_cols = [col for col in df_resp_all.columns if col.endswith('-response')]\n",
    "for col in response_cols:\n",
    "    cols.pop(cols.index(col))\n",
    "# Reinsert the response columns right after the user_input column\n",
    "for col in reversed(response_cols):\n",
    "    cols.insert(user_input_index + 1, col)\n",
    "df_resp_all = df_resp_all[cols]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "os.makedirs(ALL_RESULTS_FPATH, exist_ok=True)\n",
    "all_results_csv_fpath: str = os.path.join(ALL_RESULTS_FPATH, \n",
    "                                          config['dir_info']['all_results_file_name'])\n",
    "df_resp_all.to_csv(all_results_csv_fpath, index=False)\n",
    "df_resp_all.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
