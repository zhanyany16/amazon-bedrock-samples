{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa5bc74-7b05-46dd-9c6b-2f6195942c3b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Evaluations Model Responses using _LLM as a judge_\n",
    "---\n",
    "\n",
    "This notebook does as follows:\n",
    "\n",
    "1. Reads all the responses from the previous inference step and runs evaluations on the responses using an _LLM as a judge_ that selects the best model, corresponding best response given the question and context, and the subjective evaluation/explanation for choosing that model.\n",
    "\n",
    "1. Records metrics like the `p90, p95` latency, as well as `explanation` files as to why a given model was selected by the _LLM as a judge_ and why other's were not based on correctness and relevancy.\n",
    "\n",
    "1. Uses a _Final LLM as a summarizer_ to parse through all of the subjective evaluations/explanations provided by the _LLM as a judge_ and gives a final analysis on the trends, patterns spotted across the model performance and gives a summary of which model is preferred for a given use case/dataset\n",
    "\n",
    "*The model to be used as a judge and the final analysis summarizer can be configured in the `llm_as_a_judge_info` and the `final_analysis_summarizer` sections in the [config.yaml](config.yaml) file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3d35d-a54b-4b57-8562-dac1ed1b51ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import re\n",
    "import ray\n",
    "import json\n",
    "import glob\n",
    "import yaml\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from litellm import completion\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be5469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a51143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a77295-6987-49dd-baaf-7dc0957f1981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n",
    "\n",
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fe3ec-1987-4fe0-9a02-cd2cc98cfac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize all global variables that are used across this notebook hydrated from the `config.yaml` file\n",
    "\n",
    "# name of your csv file (containing the dataframe)\n",
    "FILE_NAME: str = config['dir_info']['dataset_file_name']\n",
    "# data directory\n",
    "DATA_DIR: str = config['dir_info']['data_dir']\n",
    "\n",
    "# result files\n",
    "INFERENCE_LATENCY_SUMMARY_FPATH = os.path.join(DATA_DIR, config['dir_info']['inference_latency_summary_fname'])\n",
    "METRICS_DIR: str = os.path.join(DATA_DIR, config['dir_info'] ['metrics'])\n",
    "JSON_TXT_FILE_PATH: str = os.path.join(METRICS_DIR, config['dir_info']['llm_comparisons_txt'])\n",
    "ALL_EXPLANATIONS_FPATH: str = os.path.join(METRICS_DIR, config['dir_info']['all_explanations'])\n",
    "FINAL_ANALYSIS_MODEL_ID: str = config['final_analysis_summarizer']\n",
    "FINAL_SUMMARY_ANALYSIS: str = os.path.join(METRICS_DIR, config['dir_info']['final_summary_analysis'])\n",
    "bedrock_model_ids: List[str] = config['bedrock_fms_to_test']\n",
    "USER_PROMPT_COL: str = config['dataset_info']['user_question_col']\n",
    "SYSTEM_PROMPT_COL: str = config['dataset_info']['system_prompt_col']\n",
    "INFERENCE_PARAMETERS: Dict = config['inference_parameters']\n",
    "ON_LIST = list(filter(None, [USER_PROMPT_COL, \n",
    "                             SYSTEM_PROMPT_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9564e87-a473-4c93-b23a-f9a3dc557b35",
   "metadata": {},
   "source": [
    "### Use _LLM as a Judge_ Evaluations\n",
    "---\n",
    "In this portion:\n",
    "\n",
    "1. Responses generated by each model are evaluated on relevance and meaning by your model of choice that acts as a `Judge`. Prompt for the model that acts as a judge be viewed and tweaked for different use cases in the: [prompt_template/](prompt_template/) directory. Edit and review this prompt based on the use case and criteria for subjective evaluation.\n",
    "\n",
    "1. The role of the model acting as a judge it to compare the responses generated by each model and the already provided responses in the source dataset (if any). It provides information on the selected model, response, and an explanation of its selection, with a detailed analysis of comparison between other responses and why it chose the one it did.\n",
    "\n",
    "*Note: For more information on the use of having a Model act as a judge, view: https://huggingface.co/learn/cookbook/en/llm_judge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ed701-f6fd-4de9-871e-ece75cd9818e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(row):\n",
    "    \"\"\"\n",
    "    This function evaluates the prompts by incorporating all of the responses generated by various models into the evaluation prompt template.\n",
    "    \"\"\"\n",
    "    eval_template: Optional[str] = None\n",
    "    processed_eval_template: Optional[str] = None\n",
    "    model_responses: List[str] = []\n",
    "    try:\n",
    "        # file path to the eval template\n",
    "        eval_template_path: str = config['llm_as_a_judge_info']['prompt_template']\n",
    "        with open(eval_template_path, \"r\") as f:\n",
    "            eval_template = f.read()\n",
    "            logger.info(f\"evaluation prompt template recorded: {eval_template}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Evaluation template not found at {eval_template_path}\")\n",
    "    for column in row.index:\n",
    "        if column.endswith(\"-response\") and column != config['dataset_info']['pre_existing_response_col']:\n",
    "            model_id = column.split(\"-response\")[0]\n",
    "            model_response = row[column]\n",
    "            model_responses.append(f\"\\n<{model_id}>\\n{model_response}\\n</{model_id}>\\n\")\n",
    "    print(f\"model_responses: {model_responses}\")\n",
    "\n",
    "    if config['dataset_info']['system_prompt_col'] is not None:\n",
    "        # if the system prompt is provided in the dataset, it is used as context\n",
    "        processed_eval_template = eval_template.format(\n",
    "            context=row[config['dataset_info']['system_prompt_col']], \n",
    "            question=row[config['dataset_info']['user_question_col']], \n",
    "            original_answer=row[config['dataset_info']['pre_existing_response_col']],\n",
    "            model_responses=\"\\n\".join(model_responses)\n",
    "        )\n",
    "    else:\n",
    "        # if the system prompt is not provided, the user column is assumed to have the context and so \n",
    "        # all the context is fit into the question itself\n",
    "        processed_eval_template = eval_template.format(\n",
    "            context=\" \", \n",
    "            question=row[config['dataset_info']['user_question_col']], \n",
    "            original_answer=row[config['dataset_info']['pre_existing_response_col']],\n",
    "            model_responses=\"\\n\".join(model_responses)\n",
    "        )\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17311b1",
   "metadata": {},
   "source": [
    "#### Retrieve all the results from the `results.csv` file generated in the _Inference Step_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318ca45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the inference results\n",
    "inference_results_file: str = os.path.join(METRICS_DIR, \n",
    "                                           config['dir_info']['all_results_file_name'])\n",
    "df_resp_all = pd.read_csv(inference_results_file)\n",
    "df_resp_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d118fe",
   "metadata": {},
   "source": [
    "### Construct the ***LLM as a Judge Prompt Template***\n",
    "---\n",
    "\n",
    "In this portion of the notebook, the prompt template that is used by the LLM as a judge is prepared. This sample contains examples of evaluation prompt templates using a Llama3 evaluation prompt template [here](model-evals/llm_as_a_judge/data/prompt_template/llama3_eval_prompt.txt). There is another example of an Anthropic Claude Evaluation prompt template [here](model-evals/llm_as_a_judge/data/prompt_template/claude_eval_prompt.txt).\n",
    "\n",
    "Information on which LLM as a judge to use can be configured in the `llm_as_a_judge_info` section of the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdbe07-cd57-4ae8-b56e-eebbcdb2cba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_resp_all is not None:\n",
    "    df_resp_all['eval_prompt'] = df_resp_all.apply(lambda r: prepare_eval_prompts(r), axis=1)\n",
    "    logger.info(\"preparing the evaluation prompt templates for the LLM judge....\")\n",
    "else:\n",
    "    logger.error(f\"Model evaluation dataset is not available to process.\")\n",
    "eval_path_df: str = os.path.join(METRICS_DIR, config['dir_info']['processed_eval_prompts'])\n",
    "df_resp_all.insert(0, 'prompt_id', df_resp_all.index)\n",
    "df_resp_all.to_csv(eval_path_df, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a0626-9c7b-4350-97dd-32d32fa17fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_resp_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6eea8e-e202-4af0-93ff-0ce85015796d",
   "metadata": {},
   "source": [
    "### Using LLM as a judge in the loop to evaluate and narrow down the responses generated by different models of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e036c-a6a8-4737-91c2-7b1af0354304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_judge_json_evaluations(model_id: str, prompt: str):\n",
    "    # represents the service name\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception = None,\n",
    "               user_prompt=None,\n",
    "               prompt = prompt,\n",
    "               completion = None,\n",
    "               # initializing to 0 since none type throws an error later, this is used to calculate price per token input/output on ODT pricing\n",
    "               completion_token_count = 0,\n",
    "               # initializing to 0 since none type throws an error later\n",
    "               prompt_token_count=0,\n",
    "               input_token_cost = None, \n",
    "               output_token_cost = None,\n",
    "               model_id = model_id)\n",
    "    \n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    parameters = config['inference_parameters']\n",
    "    temperature = parameters.get('temperature', 0.1)\n",
    "    caching = parameters.get('caching', False)\n",
    "    max_tokens = parameters.get(\"max_tokens\", 500)\n",
    "\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        logger.info(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{ \"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=temperature,\n",
    "                              max_tokens=max_tokens,\n",
    "                              caching=caching)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25e243-e022-474c-a3ea-7e367861381a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    # save all the responses from the model in a dictionary\n",
    "    resp: Dict = {}\n",
    "    print(f\"row={row}\")\n",
    "    model_id = model_info['model']\n",
    "    # create the payload for model inference\n",
    "    prompt = row['eval_prompt']\n",
    "    # generate the chapter title based on the given chapter in the prompt \n",
    "    resp = llm_judge_json_evaluations(model_id, prompt)\n",
    "    resp[config['dataset_info']['pre_existing_response_col']] = row[config['dataset_info']['pre_existing_response_col']]\n",
    "    # calculate the input and output token price for all of the calls\n",
    "    resp['input_token_cost'] = (resp['prompt_token_count']/1000) * model_info['input_tokens_pricing']\n",
    "    resp['output_token_cost'] = (resp['completion_token_count']/1000) * model_info['output_tokens_pricing']\n",
    "    dir_path = os.path.join(config['dir_info']['llm_as_a_judge_dir'], str(row['prompt_id']), model_id.replace(\":\", \"-\"))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    fpath = os.path.join(dir_path, f\"model_evaluation_{row['prompt_id']}.json\")\n",
    "    logger.info(f\"writing response={resp} to {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(resp, default=str, indent=2))\n",
    "    logger.info(f\"response {i}: {resp}\")\n",
    "    return resp\n",
    "\n",
    "@ray.remote\n",
    "def async_get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return get_inference(i, row, total, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3222bcf-4d07-4652-8f22-1fd841f63310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_resp_all = json.loads(df_resp_all.to_json(orient='records'))\n",
    "n: int = config.get('parallel_inference_count')\n",
    "resp_list: List = []\n",
    "erroneous_count = 0  # To keep track of errors\n",
    "st = time.perf_counter()\n",
    "EVAL_MODEL_INFO: Dict = config['llm_as_a_judge_info']\n",
    "logger.info(f\"------ running inference for {EVAL_MODEL_INFO.get('model')} -----\")\n",
    "\n",
    "# Split the input list\n",
    "list_of_lists = [df_resp_all[i * n:(i + 1) * n] for i in range((len(df_resp_all) + n - 1) // n)]\n",
    "logger.info(f\"split input list of size {len(df_resp_all)} into {len(list_of_lists)} lists\")\n",
    "\n",
    "# Process each list\n",
    "for idx, l in enumerate(list_of_lists):\n",
    "    try:\n",
    "        logger.info(f\"getting inference for list {idx+1}/{len(list_of_lists)}, size of list={len(l)}\")\n",
    "        resp_list.extend(ray.get([async_get_inference.remote(i + 1, e, len(l), EVAL_MODEL_INFO) for i, e in enumerate(l)]))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing list {idx+1}/{len(list_of_lists)}: {e}\")\n",
    "        erroneous_count += 1\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"------ model={EVAL_MODEL_INFO.get('model')} completed in {elapsed_time} ------\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21437f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the raw responses from the LLM as a judge evaluation\n",
    "df_resp_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f94589-d247-4eb6-bb95-40d68fc82d27",
   "metadata": {},
   "source": [
    "### Visualize `LLM as a judge` completions and get more evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136f6dc-8297-45a4-8fa6-9ac5a7ec2654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents extracted all metric files\n",
    "fpath_evaluated_files = os.path.join(config['dir_info']['llm_as_a_judge_dir'], \"**\", \"*\", \"*.json\")\n",
    "eval_metric_files = glob.glob(fpath_evaluated_files, recursive=True)\n",
    "logger.info(f\"there are {len(eval_metric_files)} evaluated files by {config['llm_as_a_judge_info']['model']} LLM judge in {fpath_evaluated_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    This function is used to clean up the data generated by the LLM as a judge to get\n",
    "    responses split out a JSON format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        question_match = re.search(r'Question:(.*?)```', text, re.DOTALL)\n",
    "        question = question_match.group(1).strip() if question_match else None\n",
    "    except Exception as e:\n",
    "        print(f\"The question was not extracted: {e}\")\n",
    "        question = None\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752ace3-f0e9-456a-9361-38f9f5744be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(config['dir_info']['metrics'], exist_ok=True)\n",
    "model_evaluation_responses = []\n",
    "\n",
    "for f in eval_metric_files:\n",
    "    with open(f, 'r') as file:\n",
    "        model_evaluation_responses.append(json.loads(file.read()))\n",
    "# results_df will contain the evaluation responses, including the completion and the model id\n",
    "results_df = pd.DataFrame(model_evaluation_responses)\n",
    "raw_llm_as_a_judge_responses: str = config['dir_info']['raw_llm_as_a_judge_completions']\n",
    "raw_llm_fpath: str = os.path.join(METRICS_DIR, raw_llm_as_a_judge_responses)\n",
    "results_df = results_df.dropna(axis=1, how='all')\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b176f-a40d-435e-bcac-527cbaf0bc16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_unescaped_quotes(pairs):\n",
    "    new_pairs = []\n",
    "    for key, value in pairs:\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"'\", r\"\\'\").replace('\"', r'\\\"')\n",
    "        new_pairs.append((key, value))\n",
    "    return dict(new_pairs)\n",
    "\n",
    "def clean_model_eval_json(data):\n",
    "    \"\"\"\n",
    "    This function takes in JSON data, cleans it, and assigns the selected title as outputted by the model evaluator.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the input string to handle unescaped double quotes at the start\n",
    "        if data.startswith('\"'):\n",
    "            data = \"'\" + data[1:-1].replace('\"', '\\\\\"') + \"'\"\n",
    "        data = data.replace('\\n', ' ')\n",
    "\n",
    "        json_data = json.loads(data, object_pairs_hook=replace_unescaped_quotes)\n",
    "        \n",
    "        # Remove angle brackets from the selected_model value\n",
    "        selected_model = json_data.get('selected_model', '')\n",
    "        json_data['selected_model'] = re.sub(r'[<>]', '', selected_model)\n",
    "\n",
    "        return pd.Series({\n",
    "            'best_match_answer': json_data.get('best_match_answer'),\n",
    "            'selected_model': json_data.get('selected_model'),\n",
    "            'explanation': json_data.get('explanation'),\n",
    "        })\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Invalid JSON data: {data} - {e}\")\n",
    "        return pd.Series({\n",
    "            'best_match_answer': None,\n",
    "            'selected_model': None,\n",
    "            'explanation': None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815cce3-03b4-4191-b142-43144942b319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tidy_split(df, column, sep=',', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09a774-e3c0-4b26-a0e3-508234c0a2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_results_df = results_df['completion'].apply(clean_model_eval_json)\n",
    "# removing any unnecessary characters from the selected_model if any\n",
    "new_results_df['selected_model'] = new_results_df['selected_model'].str.replace(r'<[^>]+>', '', regex=True)\n",
    "# here we split the elements of the selected_model column using the tidy split function\n",
    "new_exploded_df = tidy_split(new_results_df, 'selected_model', sep=',')\n",
    "new_results_df[config['dataset_info']['pre_existing_response_col']] = results_df[config['dataset_info']['pre_existing_response_col']]\n",
    "new_results_df['input_token_cost'] = results_df['input_token_cost']\n",
    "new_results_df['output_token_cost'] = results_df['output_token_cost']\n",
    "logger.info(f\"All evaluation data is read into a dataframe of shape {results_df.shape}\")\n",
    "cols = new_results_df.columns.tolist()\n",
    "idx = cols.index('selected_model')\n",
    "cols.insert(idx + 1, cols.pop(cols.index(config['dataset_info']['pre_existing_response_col'])))\n",
    "new_results_df.drop(columns=['input_token_cost', 'output_token_cost'], inplace=True)\n",
    "# display the selected title, model explanation and the respective golden title in a side by side view\n",
    "new_results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d32eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.read_csv(eval_path_df)\n",
    "# Merge the two DataFrames on 'gpt_response'\n",
    "merged_df = pd.merge(new_results_df, initial_df[[config['dataset_info']['pre_existing_response_col'], \n",
    "                                                config['dataset_info']['user_question_col']]], on=config['dataset_info']['pre_existing_response_col'], how='left')\n",
    "\n",
    "cols = [col for col in merged_df.columns if col != 'user prompt']\n",
    "processed_prompts_for_eval_path = os.path.join(METRICS_DIR, config['dir_info']['llm_as_a_judge_comparisons'])\n",
    "merged_df.to_csv(processed_prompts_for_eval_path, index=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f690320",
   "metadata": {},
   "source": [
    "### View the LLM as a judge comparison and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c548926",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_prompts_for_eval_path = os.path.join(METRICS_DIR, config['dir_info']['llm_as_a_judge_comparisons'])\n",
    "merged_df = pd.read_csv(processed_prompts_for_eval_path)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to JSON\n",
    "merged_df_json = merged_df.to_json(orient='records')\n",
    "\n",
    "# Save the JSON to a text file\n",
    "with open(JSON_TXT_FILE_PATH, 'w') as json_text_file:\n",
    "    json_text_file.write(merged_df_json)\n",
    "logger.info(f\"CSV saved to: {processed_prompts_for_eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b901c3",
   "metadata": {},
   "source": [
    "### Generate the LLM as a judge `pick rate` to show how many times a model was picked having the best response over the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417d660-9137-49bf-bad5-152ce302b8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the percentage of each model selection and reset the index\n",
    "new_exploded_df['selected_model'] = new_exploded_df['selected_model'].map(lambda x: x.strip())\n",
    "response_index_percentage_df = new_exploded_df['selected_model'].value_counts(normalize=True).reset_index()\n",
    "response_distribution_fpath = os.path.join(METRICS_DIR, config['dir_info']['llm_as_a_judge_pick_rate'])\n",
    "response_index_percentage_df['proportion'] *= 100\n",
    "response_index_percentage_df.to_csv(response_distribution_fpath, index=False)\n",
    "response_index_percentage_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c5573",
   "metadata": {},
   "source": [
    "### Final Summary: `LLM evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa952df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to get a final summary on all of the data provided from LLM as a judge\n",
    "def final_analysis_summary(bedrock: botocore.client, \n",
    "                           prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in the prompt that checks whether the text file has a response to the question and if not, \n",
    "    returns \"not found\" to move to the next hit\n",
    "    \"\"\"\n",
    "    modelId=FINAL_ANALYSIS_MODEL_ID\n",
    "    body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "        modelId=modelId,\n",
    "        body=body)\n",
    "\n",
    "        response_body = json.loads(response['body'].read().decode(\"utf-8\"))\n",
    "        llm_response = response_body['content'][0]['text'].replace('\"', \"'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        llm_response = None\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150241e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ALL_EXPLANATIONS_FPATH, 'w') as file:\n",
    "    for index, row in merged_df.iterrows():\n",
    "        file.write(f\"Selected Model: {row['selected_model']}\\nExplanation: {row['explanation']}\\n\\n\")\n",
    "\n",
    "# Read the content back to use as analysis context\n",
    "with open(ALL_EXPLANATIONS_FPATH, 'r') as file:\n",
    "    analysis_context = file.read()\n",
    "print(analysis_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b582e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the prompt template and prepare it for inference\n",
    "with open(config['dir_info']['claude_final_summary_eval_prompt'], 'r') as file:\n",
    "    final_summary_prompt = file.read()\n",
    "    processed_summary_eval_prompt: str = final_summary_prompt.format(context=analysis_context)\n",
    "\n",
    "endpoint_url: str = config['bedrock_ep_url'].format(region=config['aws']['region'])\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=endpoint_url)\n",
    "final_analysis: str = final_analysis_summary(bedrock, prompt=processed_summary_eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4db1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(FINAL_SUMMARY_ANALYSIS).write_text(final_analysis + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
